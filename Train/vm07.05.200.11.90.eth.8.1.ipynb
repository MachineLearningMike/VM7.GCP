{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "os.environ['TF_ENABLE_ONEDNN_OPTS'] = str(0)\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras  # tf.keras\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"python\", sys.version)\n",
    "for module in mpl, np, pd, sklearn, tf, keras:\n",
    "    print(module.__name__, module.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert sys.version_info >= (3, 5) # Python ≥3.5 required\n",
    "assert tf.__version__ >= \"2.0\"    # TensorFlow ≥2.0 required"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from test_utility import *"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_times = 1000; n_markets = 100; n_fields = 10\n",
    "Candles = [ [ [ time * n_markets * n_fields + market * n_fields + field for field in range(n_fields) ] for market in range(n_markets) ] for time in range(n_times)]\n",
    "Candles = np.array(Candles, dtype=np.float32)\n",
    "Times = np.array( range(Candles.shape[0]), dtype=Candles.dtype ) + 333\n",
    "Times = np.expand_dims(Times, axis=1)\n",
    "size_time = 1\n",
    "print(Candles.shape, Times.shape)   # time, market, field\n",
    "print(Candles[:2, :, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Nx = 10\n",
    "Ny = 2\n",
    "Ns = 10\n",
    "\n",
    "sample_anchors = np.array(range(0, Candles.shape[0] - Nx - Ny + 1, Ns), dtype=np.int32)\n",
    "sample_anchores_t = sample_anchors[: sample_anchors.shape[0] * 3 // 4]\n",
    "sample_anchores_v = sample_anchors[sample_anchors.shape[0] * 3 // 4 :]\n",
    "print(Candles.shape, sample_anchors.shape, sample_anchores_t.shape, sample_anchores_v.shape)\n",
    "\n",
    "x_indices = ( (0, 1, 2), (0,1,2) )    # (market, field)\n",
    "y_indices = ( (2, 3, 4), (1,2,3) )    # (market, field). Currently must be the same size as x_indices.\n",
    "print(Candles[0:2][:, x_indices[0]][:, :, x_indices[1]])\n",
    "print(Candles[2:4][:, y_indices[0]][:, :, y_indices[1]])\n",
    "\n",
    "size_x = get_timepoint_size(x_indices)\n",
    "size_y = get_timepoint_size(y_indices)\n",
    "\n",
    "print(size_x, size_y, size_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dummy variables\n",
    "CandleMarks = Candles[:, :, 0]\n",
    "all_market_names = [ 'market_' + str(i) for i in range(Candles.shape[1])]\n",
    "all_field_names = [ 'filed_' + str(i) for i in range(Candles.shape[2])]\n",
    "min_true_candle_percent_x = 50\n",
    "chosen_fields_names_x = all_field_names[2:7]\n",
    "min_true_candle_percent_y = 50\n",
    "chosen_fields_names_y = all_field_names[0:5]\n",
    "target_market_names = all_market_names[2:15] #[0:1] # [1:2]\n",
    "tarket_market_top_percent = 15\n",
    "\n",
    "assert min_true_candle_percent_x == min_true_candle_percent_y\n",
    "assert len(chosen_fields_names_x) == len(chosen_fields_names_y)\n",
    "\n",
    "Candles, CandleMarks, all_market_names, x_indices, y_indices, \\\n",
    "chosen_market_names_x, chosen_field_names_x, chosen_market_names_y, chosen_field_names_y, \\\n",
    "chosen_market_names, chosen_field_names, \\\n",
    "target_markets_names, target_markets = \\\n",
    "get_formed_data( Candles, CandleMarks, all_market_names, all_field_names, \n",
    "        min_true_candle_percent_x, chosen_fields_names_x, min_true_candle_percent_y, chosen_fields_names_y,\n",
    "        target_market_names, tarket_market_top_percent\n",
    ")\n",
    "\n",
    "print(Candles.shape)\n",
    "print(CandleMarks.shape)\n",
    "print(len(all_market_names))\n",
    "print(x_indices)\n",
    "print(y_indices)\n",
    "print(chosen_market_names_x)\n",
    "print(chosen_field_names_x)\n",
    "print(chosen_market_names_y)\n",
    "print(chosen_field_names_y)\n",
    "print(chosen_market_names)\n",
    "print(chosen_field_names)\n",
    "print(target_markets_names)\n",
    "print(target_markets)\n",
    "print(len(chosen_market_names_x), len(chosen_market_names_y), len(target_markets_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Time_into_X = True\n",
    "Time_into_Y = False\n",
    "BatchSize = 2\n",
    "shuffle_batch = 10\n",
    "\n",
    "ds_train, ds_valid, dx, dy = \\\n",
    "get_datasets_2(\n",
    "    Candles, Time_into_X, Time_into_Y, Times, \n",
    "    sample_anchores_t, sample_anchores_v,\n",
    "    Nx, x_indices, Ny, y_indices, size_time, target_markets,\n",
    "    BatchSize, shuffle_batch, shuffle=True\n",
    ")\n",
    "\n",
    "print(len(ds_train), len(ds_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_target_markets_relative = [ y_indices[0].index(i) for i in y_indices[0] if i not in target_markets ]\n",
    "print(non_target_markets_relative)\n",
    "nFeaturesPerMarket = len(y_indices[1])\n",
    "print(nFeaturesPerMarket)\n",
    "all_ntmr = []\n",
    "for ntmr in non_target_markets_relative:\n",
    "    all_ntmr = all_ntmr + list(range(ntmr* nFeaturesPerMarket, (ntmr+1) * nFeaturesPerMarket, 1))\n",
    "non_target_features_relative = tuple(all_ntmr)\n",
    "print(non_target_features_relative)\n",
    "\n",
    "def anchor_to_sample(anchor): # This function is the bottle-neck of training speed.\n",
    "    x = np.reshape(Candles[anchor: anchor + Nx][:, x_indices[0]][:, :, x_indices[1]], (Nx, -1))\n",
    "    if Time_into_X is True:\n",
    "        assert Times is not None\n",
    "        x = np.concatenate((x, np.reshape(Times[anchor: anchor + Nx], (Nx, -1))), axis=1) # concat(x, x_time)\n",
    "\n",
    "    y = np.reshape(Candles[anchor + Nx: anchor + Nx + Ny][:, y_indices[0]][:, :, y_indices[1]], (Ny, -1))\n",
    "\n",
    "    if Time_into_X is True:\n",
    "        assert Times is not None\n",
    "        y_time = np.reshape(Times[anchor + Nx: anchor + Nx + Ny], (Ny, -1))\n",
    "        if Time_into_Y is not True: y_time[:] = 0.0\n",
    "        y = np.concatenate((y, y_time), axis=1)\n",
    "\n",
    "    x = np.pad(x, [[1,1], [0,0]], constant_values=0)   # (1 pre-pad: Start, 1 post-pad: End) on axis 0. (0 pre-pad, 0 post-pad) on axis 1.\n",
    "    y = np.pad(y, [[1,1], [0,0]], constant_values=0)\n",
    "\n",
    "    if x.shape[-1] % 2 != 0:\n",
    "        x = np.pad(x, [[0,0], [0,1]], constant_values=0) # (0 pre-pad: Start, 0 post-pad: End) on axis 0. (0 pre-pad, 1 post-pad) on axis 1.\n",
    "        y = np.pad(y, [[0,0], [0,1]], constant_values=0)\n",
    "\n",
    "    y[:, non_target_features_relative] = 0.0 # Start/Stop = NoInfo = 0.0\n",
    "\n",
    "    return x, y[:-1], y[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anchor = 10\n",
    "\n",
    "x, y, y_shift = anchor_to_sample(anchor)\n",
    "print(x.shape)\n",
    "print(y.shape)\n",
    "print(y_shift.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Candles[:4, :, :]) # market order may be changed by argmax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete check of x\n",
    "\n",
    "for anchor in np.concatenate((sample_anchores_t, sample_anchores_v)):\n",
    "\n",
    "    x, y, y_shift = anchor_to_sample(anchor)\n",
    "\n",
    "    assert x.shape[0] == Nx + 2\n",
    "    size_x = len(chosen_market_names_x)  * len(chosen_field_names_x)\n",
    "    odd_padding = (1 if size_x // 2 != 0 else 0)\n",
    "    assert x.shape[1] == size_x + size_time + odd_padding\n",
    "\n",
    "    seq = 0\n",
    "    assert np.sum(x[seq]>0) == 0\n",
    "\n",
    "    for seq in range(1, x.shape[0]-1):\n",
    "        for i in range(x.shape[1] - size_time - odd_padding):\n",
    "            assert x[seq][i] == Candles[anchor + seq - 1, x_indices[0] [i//len(chosen_field_names_x)], x_indices[1] [(i%len(chosen_field_names_x))]]\n",
    "\n",
    "        if odd_padding > 0:\n",
    "            assert x[seq][-1] == 0.0     # -1 for the odd-padding\n",
    "        for i in range(size_time):\n",
    "            assert x[seq][- size_time - odd_padding + i ] == Times[anchor + seq - 1][i]\n",
    "\n",
    "    seq = x.shape[0] - 1\n",
    "    assert np.sum(x[seq]>0) == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete check of y\n",
    "\n",
    "for anchor in np.concatenate((sample_anchores_t, sample_anchores_v)):\n",
    "\n",
    "    x, y, y_shift = anchor_to_sample(anchor)\n",
    "\n",
    "    assert y.shape[0] == Ny + 1\n",
    "    assert y.shape[1] == x.shape[1]\n",
    "\n",
    "    seq = 0\n",
    "    assert np.sum(x[seq]>0) == 0\n",
    "\n",
    "    for seq in range(1, y.shape[0]):\n",
    "        for i in range(y.shape[1] - size_time - odd_padding):\n",
    "            if i//len(chosen_field_names_y) in target_markets:\n",
    "                assert y[seq][i] == Candles[anchor + Nx + seq - 1, y_indices[0] [i//len(chosen_field_names_y)], y_indices[1] [(i%len(chosen_field_names_y))]]\n",
    "            else:\n",
    "                assert y[seq][i] == 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete check of y_shift\n",
    "\n",
    "for anchor in np.concatenate((sample_anchores_t, sample_anchores_v)):\n",
    "\n",
    "    x, y, y_shift = anchor_to_sample(anchor)\n",
    "\n",
    "    assert y_shift.shape[0] == Ny + 1\n",
    "    assert y_shift.shape[1] == x.shape[1]\n",
    "\n",
    "    for seq in range(0, y_shift.shape[0]-1):\n",
    "        for i in range(y_shift.shape[1] - size_time - odd_padding):\n",
    "            if i//len(chosen_field_names_y) in target_markets:\n",
    "                assert y_shift[seq][i] == Candles[anchor + Nx + seq, y_indices[0] [i//len(chosen_field_names_y)], y_indices[1] [(i%len(chosen_field_names_y))]]\n",
    "            else:\n",
    "                assert y_shift[seq][i] == 0.0\n",
    "\n",
    "    seq = y_shift.shape[0] - 1\n",
    "    assert np.sum(y_shift[seq]>0) == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_train, ds_valid, dx, dy = \\\n",
    "get_datasets_2(\n",
    "    Candles, Time_into_X, Time_into_Y, Times, \n",
    "    sample_anchores_t, sample_anchores_v,\n",
    "    Nx, x_indices, Ny, y_indices, size_time, target_markets,\n",
    "    BatchSize, shuffle_batch, shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for elem in ds_train:\n",
    "    print(elem)\n",
    "    break"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipyparams\n",
    "notebookName = ipyparams.notebook_name  # This only works on browser.\n",
    "data_model = \"vm07.05.200.11.90.eth.8.1\"\n",
    "print(data_model)\n",
    "\n",
    "dir_data = \"/mnt/data/Trading/\"\n",
    "\n",
    "#===================================================================== Dataset\n",
    "\n",
    "Nx = 200 # ------------- test\n",
    "Ny = 11\n",
    "Ns = 5 #--------------------- test\n",
    "BatchSize = 64\n",
    "\n",
    "Shift = 3 # past: 2\n",
    "\n",
    "CandleFile = \"18-01-01-00-00-23-05-20-20-23-5m\"\n",
    "SmallSigma = 1\n",
    "LargeSigma = 30\n",
    "eFreeNoLog = True\n",
    "\n",
    "shuffle_batch = 100  # Keep it small to speed up model loading.\n",
    "\n",
    "dir_candles = os.path.join(dir_data, \"Candles\")\n",
    "\n",
    "min_true_candle_percent_x = 90\n",
    "chosen_markets_x = []\n",
    "chosen_fields_names_x = ['ClosePrice'] #, 'BaseVolume']\n",
    "min_true_candle_percent_y = 90\n",
    "assert min_true_candle_percent_x == min_true_candle_percent_y\n",
    "chosen_markets_y = []\n",
    "chosen_fields_names_y = ['ClosePrice']\n",
    "\n",
    "target_market_names = None\n",
    "# target_market_names = ['NEOUSDT', 'LTCUSDT', 'BTCUSDT', 'ETHUSDT', 'BNBUSDT', 'QTUMUSDT', 'ADAUSDT', 'XRPUSDT']\n",
    "target_market_names = ['ETHUSDT']\n",
    "tarket_market_top_percent = 15\n",
    "\n",
    "Standardization = True\n",
    "Kill_Irregulars = True  # ----------------- pls implement it\n",
    "Time_into_X = True\n",
    "Time_into_Y = False #\n",
    "eFreeNoPlot = True\n",
    "\n",
    "#======================================================================== Model\n",
    "\n",
    "Num_Layers = 8 # Wow\n",
    "Num_Heads = 1   # As we have a single GPU, and we want to a exhaustic attention.\n",
    "Factor_FF = 4\n",
    "repComplexity = 8  # Wower\n",
    "Dropout_Rate = 0.1\n",
    "\n",
    "dir_Checkpoint = os.path.join(dir_data, \"Checkpoints\")\n",
    "checkpoint_filepath = os.path.join(dir_Checkpoint, data_model)\n",
    "dir_CSVLogs = os.path.join(dir_data, \"CSVLogs\")\n",
    "csvLogger_filepath = os.path.join(dir_CSVLogs, data_model)\n",
    "\n",
    "#======================================================================== Train\n",
    "\n",
    "Epochs_Initial = 5000\n",
    "HuberThreshold = 1.0\n",
    "Checkpoint_Monitor = \"val_loss\"\n",
    "EarlyStopping_Min_Monitor = \"val_loss\"\n",
    "EarlyStopping_Patience = 30\n",
    "\n",
    "Optimizer = \"adam\"\n",
    "Learning_Rate = 0.0001  # default: 0.001\n",
    "\n",
    "#=============================================================== Checksum\n",
    "\n",
    "assert int(data_model.split('.')[1]) == int(CandleFile.split('-')[-1][:-1])\n",
    "assert int(data_model.split('.')[2]) == Nx\n",
    "assert int(data_model.split('.')[3]) == Ny\n",
    "assert int(data_model.split('.')[4]) == min_true_candle_percent_x\n",
    "assert int(data_model.split('.')[6]) == Num_Layers\n",
    "targets = data_model.split('.')[5]\n",
    "if targets.isnumeric():\n",
    "    assert target_market_names is None\n",
    "    assert int(targets) == tarket_market_top_percent\n",
    "else:\n",
    "    for target in targets.split(','):\n",
    "        assert (target+'usdt').upper() in target_market_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    print(gpus)\n",
    "    try:\n",
    "        # Currently, memory growth needs to be the same across GPUs\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "            # tf.config.experimental.set_virtual_device_configuration(\n",
    "            #     gpu,[tf.config.experimental.VirtualDeviceConfiguration(memory_limit=5120)]) # why 5120?\n",
    "            # logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "            # print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "    except RuntimeError as e:\n",
    "        # Memory growth must be set before GPUs have been initialized\n",
    "        print(e)\n",
    "\n",
    "mirrored_strategy = None\n",
    "if len(gpus) > 1: \n",
    "    mirrored_strategy = tf.distribute.MirroredStrategy()\n",
    "    Learning_Rate = Learning_Rate * len(gpus) * 3 / 4\n",
    "\n",
    "# tf.config.experimental.set_virtual_device_configuration(\n",
    "#     gpus[0],[tf.config.experimental.VirtualDeviceConfiguration(memory_limit=5120)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "import sys \n",
    "sys.path.append('..')\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upgrade_file(path):\n",
    "    with open(path, \"+tw\") as f:\n",
    "        f.write(\"# Sorry, the content is removed.\")\n",
    "        f.write(\"\\n# Please ask Mike for the content.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folders = [dir_data, dir_candles, dir_Checkpoint, dir_CSVLogs]\n",
    "for folder in folders:\n",
    "    if not os.path.isdir(folder):\n",
    "        os.mkdir(folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#==================== Load candle data into 'table' with shape of (time, markets, 10 fields) ====================\n",
    "Candles = np.load( os.path.join( dir_candles, \"table-\" + CandleFile + \".npy\") )\n",
    "Candles = np.swapaxes(Candles, 0, 1)\n",
    "Candles = Candles.astype(np.float32)    # Ugly, just confirm.\n",
    "print(\"Candles: {}\".format(Candles.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "market = 5\n",
    "Show_Price_Volume_10(Candles[:, market, :], 1, 1, 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Event_Free_Learning_Scheme_10(Candles[:, market, :], 3, 30, 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#==================== Delete 7 candle fields from 'Candles'. ====================\n",
    "# Candles.shape becomes (time, markets, ['ClosePrice', 'BaseVolume', 'BuyerBaseVolume'] )\n",
    "\n",
    "CandleMarks = Candles[:, :, 9] # keep it for later use\n",
    "Candles = np.delete(Candles, [0, 1, 2, 5, 6, 8, 9], axis = 2) # delete Open, High, Low, qVolume, #Trades, bQVolume, CandleMarks\n",
    "all_field_names = ['ClosePrice', 'BaseVolume', 'BuyerBaseVolume']\n",
    "\n",
    "assert (~np.isfinite(Candles)).any() == False\n",
    "\n",
    "table_markets = []\n",
    "with open( os.path.join( dir_candles, \"reports-\" + CandleFile + \".json\"), \"r\") as f:\n",
    "    reports = json.loads(f.read())\n",
    "print(reports[:2])\n",
    "\n",
    "all_market_names = [ s[0: s.find(':')] for s in reports if 'Success' in s ]\n",
    "assert Candles.shape[1] == len(all_market_names)\n",
    "print(Candles.shape, len(all_market_names), all_market_names[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Candles, CandleMarks, all_market_names, x_indices, y_indices, \\\n",
    "chosen_market_names_x, chosen_field_names_x, chosen_market_names_y, chosen_field_names_y, \\\n",
    "chosen_market_names, chosen_field_names, \\\n",
    "target_markets_names, target_markets = \\\n",
    "get_formed_data( Candles, CandleMarks, all_market_names, all_field_names, \n",
    "        min_true_candle_percent_x, chosen_fields_names_x, min_true_candle_percent_y, chosen_fields_names_y,\n",
    "        target_market_names, tarket_market_top_percent\n",
    ")\n",
    "\n",
    "print(Candles.shape)\n",
    "print(CandleMarks.shape)\n",
    "print(len(all_market_names))\n",
    "print(x_indices)\n",
    "print(y_indices)\n",
    "print(chosen_market_names_x)\n",
    "print(chosen_field_names_x)\n",
    "print(chosen_market_names_y)\n",
    "print(chosen_field_names_y)\n",
    "print(chosen_market_names)\n",
    "print(chosen_field_names)\n",
    "print(target_markets_names)\n",
    "print(target_markets)\n",
    "print(len(chosen_market_names_x), len(chosen_market_names_y), len(target_markets_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_ts, interval_s, timestamps_abs = get_timestamps_2(CandleFile, Candles.shape[0])\n",
    "print(start_ts, interval_s, timestamps_abs.shape, timestamps_abs[:3])\n",
    "\n",
    "Times = get_time_features(timestamps_abs)\n",
    "Times = Times.astype(Candles.dtype)\n",
    "size_time = Times.shape[1]\n",
    "\n",
    "assert Candles.shape[0] == Times.shape[0]\n",
    "print(Candles.shape, Times.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#==================== Generate event-free data into Data ====================\n",
    "# Data loses heading items.\n",
    "# Do it before: Permute Data in time\n",
    "\n",
    "alpha = 3; beta = 3 # beta is used in 'get_eFree_with_plot'. Ugly coupling.\n",
    "event_free_data_loss = 3 * ( alpha * SmallSigma + LargeSigma)\n",
    "eFree = np.zeros( (Candles.shape[0] - event_free_data_loss, len(chosen_market_names), len(chosen_field_names)), dtype = Candles.dtype )\n",
    "\n",
    "for market in range(Candles.shape[1]):\n",
    "    for field in range(Candles.shape[2]):\n",
    "        sSigma = SmallSigma\n",
    "        if all_field_names[field] == 'BaseVolume': sSigma = SmallSigma * alpha\n",
    "        P, maP, logP, log_maP, event, eventFree = \\\n",
    "        get_eFree_with_plot(all_market_names[market], all_field_names[field], Candles[:, market, field], sSigma,\n",
    "                            LargeSigma, Candles.shape[0] - event_free_data_loss, noPlot=eFreeNoPlot, noLog=eFreeNoLog)\n",
    "        assert Candles.shape[0] - event_free_data_loss == eventFree.shape[0]\n",
    "        eventFree = eventFree.astype(Candles.dtype)\n",
    "        Candles[event_free_data_loss:, market, field] = eventFree\n",
    "\n",
    "Candles = Candles[event_free_data_loss + Shift:]\n",
    "Times = Times[event_free_data_loss + Shift:]\n",
    "assert Candles.shape[0] == Times.shape[0]\n",
    "\n",
    "print(Candles.shape, Times.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Standard = None\n",
    "\n",
    "if Standardization:\n",
    "    Candles, Standard = standardize_2(Candles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(16,3))\n",
    "ax = fig.add_subplot(111)\n",
    "ax.set_title(\"Features are custom-standardized\" if Standardization else \"Features are not standardized\")\n",
    "for market in range(Candles.shape[1]):\n",
    "    for field in range(Candles.shape[2]):\n",
    "        ax.plot(Candles[:, market, field], label = \"{} @ {}\".format(all_field_names[field], all_market_names[market][:-len('USDT')]))\n",
    "ax.legend(loc = 'upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_anchores_t, sample_anchores_v = get_sample_anchors_2(Candles, Nx, Ny, Ns)\n",
    "print(sample_anchores_t.shape, sample_anchores_v.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_train, ds_valid, dx, dy = \\\n",
    "get_datasets_2(\n",
    "    Candles, Time_into_X, Time_into_Y, Times, \n",
    "    sample_anchores_t, sample_anchores_v,\n",
    "    Nx, x_indices, Ny, y_indices, size_time, target_markets,\n",
    "    BatchSize, shuffle_batch, shuffle=(len(gpus)<=1)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = ds_train.take(1)\n",
    "print(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = None\n",
    "\n",
    "if mirrored_strategy is None:\n",
    "    model = build_model_2(\n",
    "        dx, dy, Num_Layers, Num_Heads, Factor_FF, repComplexity, Dropout_Rate,\n",
    "        HuberThreshold, Optimizer, Learning_Rate\n",
    "    )\n",
    "else:\n",
    "    with mirrored_strategy.scope():\n",
    "        model = build_model_2(\n",
    "            dx, dy, Num_Layers, Num_Heads, Factor_FF, repComplexity, Dropout_Rate,\n",
    "            HuberThreshold, Optimizer, Learning_Rate\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = get_callbacks(\n",
    "    checkpoint_filepath, Checkpoint_Monitor, \n",
    "    csvLogger_filepath, \n",
    "    EarlyStopping_Min_Monitor, EarlyStopping_Patience\n",
    ")\n",
    "\n",
    "try:\n",
    "    model.load_weights(checkpoint_filepath)\n",
    "    print(\"Loading a checkpoint...\")\n",
    "except:\n",
    "    print(\"No checkpoint to load\")\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    columns = ('loss', 'val_loss', 'mTA', 'val_mTA')\n",
    "    plot_csv_train_history(csvLogger_filepath, columns, title=data_model)\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.fit(\n",
    "    ds_train, # x and y_true\n",
    "    validation_data=ds_valid,\n",
    "    epochs=1, #Epochs_Initial,\n",
    "    callbacks=callbacks\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(\n",
    "    ds_train, # x and y_true\n",
    "    validation_data=ds_valid,\n",
    "    epochs=20, #Epochs_Initial,\n",
    "    callbacks=callbacks\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ('loss', 'val_loss', 'mTA', 'val_mTA')\n",
    "plot_csv_train_history(csvLogger_filepath, columns, title=data_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(\n",
    "    ds_train, # x and y_true\n",
    "    validation_data=ds_valid,\n",
    "    epochs=50, #Epochs_Initial,\n",
    "    callbacks=callbacks\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ('loss', 'val_loss', 'mTA', 'val_mTA')\n",
    "plot_csv_train_history(csvLogger_filepath, columns, title=data_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(\n",
    "    ds_train, # x and y_true\n",
    "    validation_data=ds_valid,\n",
    "    epochs=500, #Epochs_Initial,\n",
    "    callbacks=callbacks\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ('loss', 'val_loss', 'mTA', 'val_mTA')\n",
    "plot_csv_train_history(csvLogger_filepath, columns, title=data_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.fit(\n",
    "    ds_train, # x and y_true\n",
    "    validation_data=ds_valid,\n",
    "    epochs=500, #Epochs_Initial,\n",
    "    callbacks=callbacks\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ('loss', 'val_loss', 'mTA', 'val_mTA')\n",
    "plot_csv_train_history(csvLogger_filepath, columns, title=data_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(\n",
    "    ds_train, # x and y_true\n",
    "    validation_data=ds_valid,\n",
    "    epochs=500, #Epochs_Initial,\n",
    "    callbacks=callbacks\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ('loss', 'val_loss', 'mTA', 'val_mTA')\n",
    "plot_csv_train_history(csvLogger_filepath, columns, title=data_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
